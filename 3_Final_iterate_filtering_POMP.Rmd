---
title: "Iterated filtering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Iterated Filtering
- repeated particle filtering options, with **randomly perturbed parameter values**, in order to **maximize** the likelihood, instead of direct particle filtering with constant parameter values
- only **full-information, plug and play, frequentist methods**
- parameters for each particle filter carried out a **stochaistic perturbation** (random walk)
- each iteration repeating a particle filter, carried out a **decreasing perturbation**, by applying the collection of parameters vectors at end of a iteration as the starting parameters to the new iteration.

### Data Visualization 
- first 42 weeks only
```{r}
library(tidyverse)
read_csv(paste0("https://kingaa.github.io/sbied/stochsim/",
                "Measles_Consett_1948.csv")) %>%
  select(week,reports=cases) %>%
  filter(week <= 42) -> dat

dat %>%
  ggplot(aes(x=week,y=reports)) +
  geom_line()
```

### Impletation of Stochastic SIR model with pomp
- basic SIR Markov chain
- assume 1 infection at week 0
- having one fit model, one should certainly examine **alternative mode**
  - ex. latency period, better description of the diagonsis, bed-confinement, etc.
- We **do not track R**, since the compartment has no consequences for the dynamics of the state nor for the data
```{r}

library(pomp)

sir_step <- Csnippet("
  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));
  double dN_IR = rbinom(I, 1-exp(-mu_IR*dt));
  S -= dN_SI;
  I += dN_SI - dN_IR;
  H += dN_IR;"
  )

sir_init <- Csnippet("
  S = nearbyint(eta*N);
  I=1;
  H=0;"
  )

dmeas <- Csnippet("
  lik = dbinom(reports, H, rho,give_log);"
  )

rmeas <- Csnippet("
  reports = rbinom(H,rho);"
  )

dat %>%
  pomp(
    times="week",t0=0,
    rprocess = euler(sir_step,delta.t = 1/7),
               rinit=sir_init,
               rmeasure=rmeas,
               dmeasure=dmeas,
               accumvars="H",
               statenames=c("S","I","H"),
               paramnames=c("Beta","mu_IR","eta","rho","N")
                     ) -> measSIR
  
```

**Testing code**
- run some simulation and particle filter
- we will use params from previous exploration
```{r}
params <- c(Beta=20, mu_IR=2, rho=0.5, eta=0.1,N=38000)

# Testing with simulation
measSIR %>% 
  simulate(params=params, nsim=10, format="data.frame") -> y

y %>% 
  ggplot(aes(x=week,y=reports,group=.id,color=factor(.id)))+
  geom_line()+
  scale_color_brewer(type = "qual",palette=3)+
  guides(color=FALSE)

# Testing with particle filtering 
measSIR %>%
  pfilter(Np=1000,params=params) ->
  pf

plot(pf)# data, effective sample size of the particle filters (ess) = number of independent particles, log likelihood of each observation conditional on the preceding ones (cond.logLok)

```

### Setting up the estimation problem
- assume population size is known, N
- assume we can estimate infectious period  to 3.5 days, thus rate of recovery, u_IR= 1/.5wks = 2wk^-1
```{r}

fixed_params <- c(N=38000, mu_IR=2)
 
# set up parallel computing
library(foreach)
library(doParallel)
registerDoParallel()
library(doRNG)
registerDoRNG(625904618)

# Testing with direct replication of particle filtering
foreach(i=1:10, .combine=c) %dopar% {
  library(pomp)
  measSIR %>% pfilter(params=params, Np=10000)
} -> pf

# estimate avg loglikelihood and standard error of the replicated pfilter simulation
pf[[1]] %>% coef() #parameter values after replicated pfilter simulation


# estimate the log likelihood of this param vector
pf %>% logLik() %>% logmeanexp(se=TRUE) -> L_pf  
L_pf # with se under 1  log unit, the simulation is not bad
```

### Building up a picture of likelihood surface
- storing likelihood estimate whenever we compute them
- set up a **database** to store likelihood of every point we estimated likelihood on
```{r}
# set up db
log_like_db <- "/Users/rachel/Documents/Github/SISMID2020/measles_params.csv"

# save param values along with log likelihood as well as se into the database
# first one from previous direct particle filter replication simulation
pf[[1]] %>% coef() %>% bind_rows() %>%
  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) %>%
  write.csv(log_like_db)
```

### Local search on the likelihood surface with Iterated Filtering
- **log likelihood estimation** from each iterated filtering operation will be recorded in the **database** created in the previous block.
- mif2
We need to choose:
- **rw.sd**, random walk magnitude of perturbation
- **cooling fraction.50**, standard RW magnitude decreasing after 50 iteration to this fraction
- estimate params on the **log scale** to constrain the params to be **postive**
- use perturbation size = 0.02, same for both params Beta and mu_IR
```{r}

foreach(i=1:20,.combine=c) %dopar% { # 20 independent iterative filtering operations
  
  library(pomp)
  library(tidyverse)
  measSIR %>%
    mif2(
      params=params,
      Np=2000, Nmif=50,# particle filter with 2000 particles, replicated with iterated filtering for 50 replications
      partrans=parameter_trans(
        log = c("Beta"),
        logit = c("rho","eta"),
      ), # param transformation
      paramnames = c("Beta","rho","eta"),
      cooling.fraction.50=0.5,
      rw.sd=rw.sd(Beta=0.02,rho=0.02,eta=ivp(0.02)) # ivp, initial value parameter, perturbation only apply at the 1st (beginning) of the time series
    ) 
} -> mifs_local

```

### Iterated filtering diagnostics
```{r}
mifs_local %>%
  traces() %>%
  melt() %>%
  ggplot(aes(x=iteration,y=value,group=L1,color=factor(L1))) +
  geom_line() +
  guides(color=FALSE) +
  facet_wrap(~variable,scales="free_y")

```

### Estimating the likelihood
- likelihood estimation at the final filtering operation generated a likelihood estimation at the resulting point, but this estimation is **not good enough for a reliable inference**
  - the **parameter perturbations** was also applied in the **last filtering iteration**
  - mif2 operations were carried out with fewer particles (Np) than are needed for a good likelihood evaluation.(refer to **Exercise 3.3**, standard error caused by different numbers of Nps)
- Therefore, we **evaulate the likelihood with standard error** using replicated particle filters at **each point estimate**
```{r}

foreach(mf=mifs_local,.combine=rbind) %dopar% {
  library(pomp)
  library(tidyverse)
  # add 20000 more particles at each point estimated at iterated operations
  # evaulate the loglikelihood of each resulting point 
  # repeat particle filtering for all iterated filtered points for 10 times w/o perturbation to expore the variance
  evals <- replicate(10,logLik(pfilter(mf,Np=20000))) 
  ll <- logmeanexp(evals,se=TRUE) # calculate loglike and standard error for the replicated 
  mf %>% coef() %>% bind_rows() %>% 
    bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results # 20 loglikelihood from iterated filtering operations


```

### Visualize the parameter values from each iterated filtering operations after no perturbated replications with additional particles
```{r}
pairs(~loglik + Beta + eta + rho , data = results, pch = 16)

```


add these loglikelihood estimations to the **database**
```{r}
read_csv(log_like_db) %>%
  bind_rows(results) %>%
  arrange(-loglik) %>%
  write_csv(log_like_db)


```

---

## MLE

### global search
- **parameter estimation** for dynamic system requires both:
  - **initial values**: state of system at t0 (state space)
  - **starting values**: parameter values a search is initialized (parameter space)
- parameter estimation involves trying many starting values or choose a **large box** in parameter space that contains all remotely sensible parameter vectors

### Design box containing reasonable parameter values
```{r}
set.seed(2062379496)

# reasonable parameter box for parameter space searching
runifDesign(
  lower=c(Beta=5,rho=0.2, eta=0),
  upper=c(Beta=80,rho=0.9,eta=0.4),
  nseq=300
) -> guesses

head(guesses)
# choose a parameter vector from iterated filtering 
# search results and model settings from iterated filtering
mf1 <- mifs_local[[1]] 

```
### iterating through all starting values in the box designed
- all parameter vectors in the box will be used as the starting values for the parameter space search
- iterated filtering (IF) will search through the parameter space with perturbation (random walk)
- algorithmic arguments perserved from mf1 will be used on mif2
```{r}
registerDoRNG(1270401374)


foreach(guess=iter(guesses,"row"),.combine=rbind) %dopar% {
  library(pomp)
  library(tidyverse)
  
  mf1 %>% # search results and model settings from iterated filtering
    mif2(params=c(unlist(guess),fixed_params)) %>% mif2(Nmif=100) -> mf # each row of param vectors of guesses
    # last step already had 50 iterations of IF, added 100 more iterations starting from end of last step (with parameter perturbation), all algorithmic parameters are perserved
    
  
  replicate(10,mf %>% pfilter(Np=100000) %>% logLik() #do more pfilter direct replication without parameter perturbation
            ) %>%
    logmeanexp(se=TRUE) -> ll # avg loglik and standard error
  
  mf %>% coef() %>% bind_rows() %>%
    bind_cols(loglik=ll[1],loglik.se=ll[2]) # add the estimated loglik to the database
} -> results

```
### Visualize global geometry of the likelihood surface
- grey: starting values, red: end results
- Optimization attempts from diverse remote staring points **converge** on a particular region in the parameter space
```{r}

read_csv(log_like_db) %>%
  filter(loglik > max(loglik) -50) %>% # all the points within the range 0f 50 unit less than MLE
  bind_rows(guesses) %>%
  mutate(type=if_else(is.na(loglik),"guess","result")) %>%
  arrange(type) -> all

pairs(~loglik + Beta + eta + rho, data = all,
      col=ifelse(all$type=="guess", grey(0.5),"red"),pch=16)
```

### Poor Man's Profile
- **global search** projections of the parameter estimates
```{r}
all %>% 
  filter(type=="results") %>%
  filter(loglik > max(loglik) -50 ) %>%
  ggplot(aes(x=eta, y=loglik)) +
  geom_point() +
  labs(
    x=expression("eta"),
    title="Poor man's profile likelihood"
  )

```